name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-D warnings"

jobs:
  # ── Build + Unit + Integration ──────────────────────────────────────
  test:
    name: Build & Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Clippy
        run: cargo clippy --workspace --all-targets -- -D warnings

      - name: Build workspace
        run: cargo build --workspace

      - name: Unit + integration tests
        run: cargo test --workspace
        env:
          DATABASE_URL: "sqlite:///tmp/ci_test.sqlite3"
          STORAGE_ROOT: "/tmp/ci_storage"
          AGENT_NAME: "CiTestAgent"
          HTTP_HOST: "127.0.0.1"
          HTTP_PORT: "1"
          HTTP_PATH: "/mcp/"

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts
          path: tests/artifacts/
          retention-days: 14
          if-no-files-found: ignore

  # ── Mode matrix harness (Rust integration) ──────────────────────────
  mode-matrix:
    name: Mode Matrix
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build both binaries
        run: |
          cargo build -p mcp-agent-mail-cli
          cargo build -p mcp-agent-mail

      - name: Mode matrix harness (Rust)
        run: cargo test -p mcp-agent-mail-cli --test mode_matrix_harness -- --nocapture
        env:
          DATABASE_URL: "sqlite:///tmp/ci_matrix.sqlite3"
          STORAGE_ROOT: "/tmp/ci_storage"
          AGENT_NAME: "CiTestAgent"
          HTTP_HOST: "127.0.0.1"
          HTTP_PORT: "1"
          HTTP_PATH: "/mcp/"

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload matrix artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mode-matrix-artifacts
          path: tests/artifacts/cli/mode_matrix/
          retention-days: 14
          if-no-files-found: ignore

  # ── Cross-platform native command matrix ───────────────────────────
  native-command-matrix:
    name: Native Command Matrix (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    needs: test
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build CLI binary
        run: cargo build -p mcp-agent-mail-cli

      - name: Run native command portability matrix
        shell: bash
        run: |
          set -euo pipefail

          AM_BIN="target/debug/am"
          ARTIFACT_ROOT="tests/artifacts/cli/native_command_matrix/${{ matrix.os }}"
          mkdir -p "${ARTIFACT_ROOT}/commands"
          export ARTIFACT_ROOT

          PYTHON_BIN="python3"
          if ! command -v "${PYTHON_BIN}" >/dev/null 2>&1; then
            PYTHON_BIN="python"
          fi

          cat > "${ARTIFACT_ROOT}/environment.json" <<EOF
          {
            "runner_os": "${{ runner.os }}",
            "runner_arch": "${{ runner.arch }}",
            "matrix_os": "${{ matrix.os }}",
            "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

          run_case() {
            local case_id="$1"
            shift
            local case_dir="${ARTIFACT_ROOT}/commands/${case_id}"
            mkdir -p "${case_dir}"

            printf '%q ' "$@" > "${case_dir}/command.txt"
            printf '\n' >> "${case_dir}/command.txt"

            local started_ns ended_ns elapsed_ms exit_code
            started_ns="$("${PYTHON_BIN}" -c 'import time; print(time.time_ns())')"
            if "$@" > "${case_dir}/stdout.txt" 2> "${case_dir}/stderr.txt"; then
              exit_code=0
            else
              exit_code=$?
            fi
            ended_ns="$("${PYTHON_BIN}" -c 'import time; print(time.time_ns())')"
            elapsed_ms="$("${PYTHON_BIN}" -c "print(round((${ended_ns}-${started_ns})/1_000_000, 3))")"

            printf '%s\n' "${exit_code}" > "${case_dir}/exit_code.txt"
            printf '%s\n' "${elapsed_ms}" > "${case_dir}/timing_ms.txt"

            if [ "${exit_code}" -ne 0 ]; then
              echo "Command case ${case_id} failed with exit ${exit_code}"
              return 1
            fi
          }

          run_case ci_help "${AM_BIN}" ci --help
          run_case flake_triage_help "${AM_BIN}" flake-triage --help
          run_case bench_help "${AM_BIN}" bench --help
          run_case check_inbox_help "${AM_BIN}" check-inbox --help
          run_case golden_verify_help "${AM_BIN}" golden verify --help
          run_case serve_http_help "${AM_BIN}" serve-http --help
          run_case e2e_run_help "${AM_BIN}" e2e run --help
          run_case share_wizard_help "${AM_BIN}" share wizard --help
          run_case share_verify_live_help "${AM_BIN}" share deploy verify-live --help
          run_case doctor_check_help "${AM_BIN}" doctor check --help
          run_case check_inbox_direct_json "${AM_BIN}" check-inbox --direct --json --project "${PWD}" --agent MatrixProbe --rate-limit 0
          run_case config_set_port_envfile "${AM_BIN}" config set-port 8765 --env-file "${ARTIFACT_ROOT}/tmp env file.env"
          run_case config_show_port "${AM_BIN}" config show-port

          "${PYTHON_BIN}" - <<'PY'
          import json
          import os
          from pathlib import Path

          root = Path(os.environ["ARTIFACT_ROOT"])
          command_root = root / "commands"
          cases = []
          failures = 0

          for case_dir in sorted(p for p in command_root.iterdir() if p.is_dir()):
              exit_code = int((case_dir / "exit_code.txt").read_text(encoding="utf-8").strip())
              elapsed_ms = float((case_dir / "timing_ms.txt").read_text(encoding="utf-8").strip())
              cases.append(
                  {
                      "id": case_dir.name,
                      "exit_code": exit_code,
                      "elapsed_ms": elapsed_ms,
                  }
              )
              if exit_code != 0:
                  failures += 1

          summary = {
              "total": len(cases),
              "pass": len(cases) - failures,
              "fail": failures,
              "cases": cases,
          }
          (root / "summary.json").write_text(json.dumps(summary, indent=2) + "\n", encoding="utf-8")

          if failures:
              raise SystemExit(1)
          PY

      - name: Upload native matrix artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: native-command-matrix-${{ matrix.os }}
          path: tests/artifacts/cli/native_command_matrix/${{ matrix.os }}/
          retention-days: 14
          if-no-files-found: error

  # ── Semantic conformance ────────────────────────────────────────────
  conformance:
    name: Semantic Conformance
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build CLI binary
        run: cargo build -p mcp-agent-mail-cli

      - name: Semantic conformance suite
        run: cargo test -p mcp-agent-mail-cli --test semantic_conformance -- --nocapture
        env:
          DATABASE_URL: "sqlite:///tmp/ci_conform.sqlite3"
          STORAGE_ROOT: "/tmp/ci_storage"
          AGENT_NAME: "CiTestAgent"
          HTTP_HOST: "127.0.0.1"
          HTTP_PORT: "1"
          HTTP_PATH: "/mcp/"

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload conformance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: conformance-artifacts
          path: tests/artifacts/cli/semantic_conformance/
          retention-days: 14
          if-no-files-found: ignore

  # ── Perf + security regressions ─────────────────────────────────────
  perf-security:
    name: Perf & Security
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build both binaries
        run: |
          cargo build -p mcp-agent-mail-cli
          cargo build -p mcp-agent-mail

      - name: Perf + security regression suite
        run: cargo test -p mcp-agent-mail-cli --test perf_security_regressions -- --nocapture
        env:
          DATABASE_URL: "sqlite:///tmp/ci_perf.sqlite3"
          STORAGE_ROOT: "/tmp/ci_storage"
          AGENT_NAME: "CiTestAgent"
          HTTP_HOST: "127.0.0.1"
          HTTP_PORT: "1"
          HTTP_PATH: "/mcp/"

      - name: Soak harness perf trend suite (quick)
        run: am e2e run --project . soak_harness
        env:
          SOAK_SEED: "42"
          SOAK_PROJECTS: "3"
          SOAK_AGENTS_PER_PROJECT: "3"
          SUSTAINED_LOAD_RPS: "60"
          SUSTAINED_LOAD_SECS: "5"
          SOAK_DURATION_SECS: "5"

      - name: Verify perf trend artifacts
        if: always()
        run: |
          set -euo pipefail
          TREND_FILE="$(command find tests/artifacts/cli/perf_security -type f -name 'perf_timeseries.jsonl' -print -quit || true)"
          if [ -z "${TREND_FILE}" ]; then
            echo "Missing perf trend artifact (perf_timeseries.jsonl)"
            exit 1
          fi
          echo "Perf trend artifact: ${TREND_FILE}"

      - name: Verify soak trend artifacts
        if: always()
        run: |
          set -euo pipefail
          SOAK_TREND_FILE="$(command find tests/artifacts/perf/soak_harness -type f -name 'perf_timeseries.jsonl' -print -quit || true)"
          if [ -z "${SOAK_TREND_FILE}" ]; then
            echo "Missing soak trend artifact (perf_timeseries.jsonl)"
            exit 1
          fi
          echo "Soak trend artifact: ${SOAK_TREND_FILE}"

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload perf/security artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-security-artifacts
          path: |
            tests/artifacts/cli/perf_security/
            tests/artifacts/perf/soak_harness/
            tests/artifacts/soak/multi_project/
            tests/artifacts/tui/soak_replay/
          retention-days: 14
          if-no-files-found: ignore

  # ── E2E dual-mode native suite ──────────────────────────────────────
  e2e-dual-mode:
    name: E2E Dual-Mode (Native)
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build both binaries
        run: |
          cargo build -p mcp-agent-mail-cli
          cargo build -p mcp-agent-mail

      - name: E2E dual-mode suite
        run: am e2e run --project . dual_mode

      - name: E2E mode matrix suite
        run: am e2e run --project . mode_matrix

      - name: E2E web parity suite (HTTP wrapper, includes mail_ui)
        run: am e2e run --project . http

      - name: E2E static export conformance suite
        run: am e2e run --project . share

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload E2E artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts
          path: tests/artifacts/
          retention-days: 14
          if-no-files-found: ignore

  # ── Search V3 E2E + diagnostics contract ──────────────────────────────
  search-v3-e2e:
    name: Search V3 E2E + Diagnostics
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build both binaries
        run: |
          cargo build -p mcp-agent-mail-cli
          cargo build -p mcp-agent-mail

      - name: Run mandatory Search V3 E2E suites
        run: |
          set -euo pipefail
          mkdir -p tests/artifacts/search_v3_ci

          # Native runner dispatches all 5 suites and reports results.
          # Environment variables pass through to underlying test scripts.
          if am e2e run --project . \
            search_v3_stdio \
            search_v3_http \
            search_v3_shadow_parity \
            search_v3_resilience \
            search_v3_load_concurrency \
            2>&1 | tee tests/artifacts/search_v3_ci/native_runner.log; then
            FAILED=0
          else
            FAILED=1
          fi

          cat > tests/artifacts/search_v3_ci/required_suites.txt <<'EOF'
          search_v3_stdio
          search_v3_http
          search_v3_shadow_parity
          search_v3_resilience
          search_v3_load_concurrency
          EOF

          echo "SEARCH_V3_E2E_FAILED=${FAILED}" >> "${GITHUB_ENV}"
        env:
          E2E_CLOCK_MODE: "deterministic"
          E2E_SEED: "424242"
          SEARCH_V3_LOG_ROOT: "${{ github.workspace }}/tests/artifacts/search_v3"
          SV3_ARTIFACT_ROOT: "${{ github.workspace }}/tests/artifacts/search_v3"

      - name: Run Search V3 diagnostics rust tests
        if: always()
        run: |
          set -euo pipefail
          mkdir -p tests/artifacts/search_v3_ci
          FAILED=0

          run_diag_test() {
            local test_name="$1"
            local log_file="tests/artifacts/search_v3_ci/${test_name}.log"
            echo "::group::${test_name}"
            if cargo test -p mcp-agent-mail-db --test "${test_name}" -- --nocapture 2>&1 | tee "${log_file}"; then
              echo "PASS: ${test_name}"
            else
              echo "FAIL: ${test_name}"
              FAILED=1
            fi
            echo "::endgroup::"
          }

          run_diag_test filter_pagination
          run_diag_test logging_redaction
          run_diag_test diversity_dedup
          run_diag_test timeout_backpressure

          echo "SEARCH_V3_DIAG_FAILED=${FAILED}" >> "${GITHUB_ENV}"

      - name: Verify Search V3 artifact + assertion contract
        if: always()
        run: |
          set -euo pipefail
          python3 <<'PY'
          import json
          import sys
          from pathlib import Path

          artifact_root = Path("tests/artifacts")
          errors: list[str] = []

          floors = {
              "search_v3_stdio": 80,
              "search_v3_http": 90,
              "search_v3_shadow_parity": 1,
              "search_v3_resilience": 70,
              "search_v3_load_concurrency": 60,
          }

          def newest(paths):
              return sorted(paths, key=lambda p: str(p))[-1] if paths else None

          for suite, floor in floors.items():
              summary = newest((artifact_root / suite).glob("*/summary.json"))
              if summary is None:
                  errors.append(f"{suite}: missing tests/artifacts/{suite}/*/summary.json")
                  continue
              data = json.loads(summary.read_text(encoding="utf-8"))
              total = int(data.get("total", -1))
              failed = int(data.get("fail", -1))
              if total < floor:
                  errors.append(f"{suite}: assertion floor violated ({total} < {floor}) in {summary}")
              if failed != 0:
                  errors.append(f"{suite}: non-zero failures ({failed}) in {summary}")

          structured_expectations = {
              "search_v3_stdio": [
                  "summaries/suite_summary.json",
                  "summaries/suite_summary.txt",
                  "logs/summary.log",
              ],
              "search_v3_http": [
                  "summaries/suite_summary.json",
                  "summaries/suite_summary.txt",
                  "logs/summary.log",
              ],
              "search_v3_shadow_parity": [
                  "summaries/suite_summary.json",
                  "summaries/suite_summary.txt",
                  "logs/summary.log",
              ],
              "search_v3_load_concurrency": [
                  "summaries/suite_summary.json",
                  "summaries/suite_summary.txt",
                  "logs/summary.log",
              ],
              "search_v3_resilience": [
                  "summary.json",
                  "summary.txt",
                  "run_metadata.json",
              ],
          }

          sv3_root = artifact_root / "search_v3"
          for suite, rel_files in structured_expectations.items():
              run_root = newest((sv3_root / suite).glob("*/"))
              if run_root is None:
                  errors.append(f"{suite}: missing structured root under {sv3_root / suite}")
                  continue
              for rel in rel_files:
                  target = run_root / rel
                  if not target.is_file():
                      errors.append(f"{suite}: missing structured artifact {target}")

          for test_name in ("filter_pagination", "logging_redaction", "diversity_dedup", "timeout_backpressure"):
              log_path = artifact_root / "search_v3_ci" / f"{test_name}.log"
              if not log_path.is_file():
                  errors.append(f"missing diagnostics test log: {log_path}")

          if errors:
              print("Search V3 CI contract verification failed:")
              for err in errors:
                  print(f" - {err}")
              sys.exit(1)

          print("Search V3 CI contract verification passed.")
          PY

      - name: Enforce Search V3 pass/fail contract
        if: always()
        run: |
          set -euo pipefail
          E2E_FAILED="${SEARCH_V3_E2E_FAILED:-1}"
          DIAG_FAILED="${SEARCH_V3_DIAG_FAILED:-1}"
          echo "SEARCH_V3_E2E_FAILED=${E2E_FAILED}"
          echo "SEARCH_V3_DIAG_FAILED=${DIAG_FAILED}"
          if [ "${E2E_FAILED}" -ne 0 ] || [ "${DIAG_FAILED}" -ne 0 ]; then
            echo "Search V3 suites/tests reported failures."
            exit 1
          fi

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload Search V3 artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: search-v3-artifacts
          path: |
            tests/artifacts/search_v3_*/
            tests/artifacts/search_v3/
            tests/artifacts/search_v3_ci/
          retention-days: 14
          if-no-files-found: error

  # ── Help snapshots (golden contract drift detection) ────────────────
  snapshots:
    name: Help Snapshots
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build CLI binary
        run: cargo build -p mcp-agent-mail-cli

      - name: Verify help snapshots
        run: cargo test -p mcp-agent-mail-cli --test help_snapshots -- --nocapture
        env:
          COLUMNS: "120"
          DATABASE_URL: "sqlite:///tmp/ci_snap.sqlite3"
          STORAGE_ROOT: "/tmp/ci_storage"
          AGENT_NAME: "CiTestAgent"
          HTTP_HOST: "127.0.0.1"
          HTTP_PORT: "1"
          HTTP_PATH: "/mcp/"

      - name: Validate artifact bundles
        if: always()
        run: |
          set -euo pipefail
          source scripts/e2e_lib.sh
          e2e_validate_bundle_tree tests/artifacts

      - name: Upload snapshot diff artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: snapshot-diff-artifacts
          path: tests/artifacts/cli/help/
          retention-days: 14
          if-no-files-found: ignore

  # ── Skip guard ──────────────────────────────────────────────────────
  # Prevents silent test-skip regressions by asserting non-zero test counts.
  skip-guard:
    name: Skip Guard
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test, mode-matrix, native-command-matrix, conformance, perf-security, e2e-dual-mode, search-v3-e2e, snapshots]
    if: always()
    steps:
      - name: Verify all required jobs passed
        run: |
          echo "Job results:"
          echo "  test: ${{ needs.test.result }}"
          echo "  mode-matrix: ${{ needs.mode-matrix.result }}"
          echo "  native-command-matrix: ${{ needs.native-command-matrix.result }}"
          echo "  conformance: ${{ needs.conformance.result }}"
          echo "  perf-security: ${{ needs.perf-security.result }}"
          echo "  e2e-dual-mode: ${{ needs.e2e-dual-mode.result }}"
          echo "  search-v3-e2e: ${{ needs.search-v3-e2e.result }}"
          echo "  snapshots: ${{ needs.snapshots.result }}"

          FAILED=0
          for result in \
            "${{ needs.test.result }}" \
            "${{ needs.mode-matrix.result }}" \
            "${{ needs.native-command-matrix.result }}" \
            "${{ needs.conformance.result }}" \
            "${{ needs.perf-security.result }}" \
            "${{ needs.e2e-dual-mode.result }}" \
            "${{ needs.search-v3-e2e.result }}" \
            "${{ needs.snapshots.result }}"; do
            if [ "$result" != "success" ]; then
              echo "FAIL: job result = $result"
              FAILED=1
            fi
          done

          if [ "$FAILED" -eq 1 ]; then
            echo "One or more required jobs did not succeed."
            exit 1
          fi
          echo "All required jobs passed."
